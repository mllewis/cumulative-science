---
title: "MAJIC Study - Results Section"
author: "Mike Frank"
date: "October 7, 2016"
output: 
  html_document:
    highlight: tango
    theme: spacelab
---
  
```{r include=FALSE}
library(tidyverse)
library(langcog)
library(lme4)
library(knitr)
library(forcats)
library(broom)
library(purrr)
opts_chunk$set(cache=FALSE, warn=FALSE, error=FALSE, message=FALSE, echo=FALSE)
```

```{r}
#setwd("/Users/mollylewis/Documents/research/Projects/2_published/majic")
theme_set(theme_bw() + 
            theme(panel.grid = element_blank(),
                  strip.background = element_blank(), 
                  legend.position = "bottom"))
# font <- "Open Sans"
```

```{r}
d <- read_csv("data/processed_data.csv") %>%
  select(-arithmeticTotal) %>%
  mutate(time = fct_recode(factor(year), "Pre" = "2015", "Post" = "2016"),
         grade = fct_recode(grade, "First Grade" = "first grade",
                            "Second Grade" = "second grade"),
         group = fct_recode(group, "Control" = "CNTL", "Mental Abacus" = "MA"),
         woodcockTotal = woodcockTotal/25, # problems on the first page
         ravens = ravens / 36, # total questions
         swm = swm / 10) %>% # arbitrary
  gather(task, score, pvAvg, arithmeticAverage, woodcockTotal, 
         gonogo, ravens, swm) %>%
  mutate(task = fct_recode(task, 
                           "Arithmetic" = "arithmeticAverage",
                           "Place Value" = "pvAvg", 
                           "WJ III" = "woodcockTotal", 
                           "Matrix Reasoning" = "ravens", 
                           "Go/No Go" = "gonogo", 
                           "Spatial WM" = "swm"), 
         task = fct_relevel(task, c("Arithmetic","Place Value","WJ III", 
                              "Matrix Reasoning", "Go/No Go","Spatial WM")))

write_csv(d, "/Users/mollylewis/Documents/research/Mentoring:Teaching/teaching/modern_research_methods/MRM_materials/slides/MRM_slides_4b/data/tidy_majic_data.csv")
```

We begin by presenting brief descriptive results. Next we report our primary analyses of intervention success. We end with a number of secondary, exploratory analyses.

**Descriptive analyses**

We had six primary outcome variables, corresponding to our six tasks: three in mathematics (Arithmetic, Place Value, and the standardized WJ III assessment) and three cognitive measures (Matrix Reasoning, Go/No Go, and Spatial Working Memory). The distribution of each variable is shown in Figure 2. 

As is evident from the Figure, all measures were higher for second graders than for first graders, and all measures showed positive growth over the course of the school year. Some showed larger changes than others due to features of the tasks themselves. For example, the place value measure was explicitly designed to capture content being learned during these two years of schooling and showed substantial movement. (Its distribution was also idiosynractic because an understanding of two-place place-value would allow a student to complete a particular subset of questions.) In contrast, the Go/No Go and Spatial WM tasks showed smaller changes relative to the amount of individual variation that we saw. 

```{r fig.width=8, fig.height=4, fig.cap="Figure 2. Histograms showing the distribution of scores from each task in our battery, split by grade level. Dashed lines show means. Upper panels show pre-test scores, lower panels show post-test scores."}

ms <- d %>%
  group_by(task, time, grade) %>%
  summarise(mean = mean(score, na.rm=TRUE)) 

ggplot(d, aes(x = score, fill = grade)) + 
  geom_histogram(binwidth=.1, alpha = .8) + 
  facet_grid(time~task) + 
  scale_fill_solarized(name = "Grade") + 
  geom_vline(data = ms, aes(xintercept = mean, col = grade), lty = 2) + 
  scale_color_solarized(guide = FALSE) + 
  ylab("Number of Students") + 
  scale_x_continuous(breaks = c(0, .5, 1)) + 
  xlab("Score")
```


```{r fig.cap="Table 2. Correlations between pre- and post-test scores for each task and grade."}
cors <- d %>% 
  select(-year) %>%
  spread(time, score) %>%
  split(interaction(.$grade, .$task)) %>%
  map_df(function(x) {
    ct <- cor.test(x$Pre, x$Post)
    y = data_frame(task = x$task[1], 
                   grade = x$grade[1], 
                   r = ct$estimate,
                   ci_lower = ct$conf.int[1], 
                   ci_upper = ct$conf.int[2], 
                   p = ct$p.value)
    return(y)}) %>%
  rename(Grade = grade, 
         Task = task, 
         "lower 95% CI" = ci_lower, 
         "upper 95% CI" = ci_upper) %>%
  mutate(Grade = fct_recode(Grade, "1st" = "First Grade", "2nd" = "Second Grade"))

kable(cors, digits = c(NA, NA, 2, 2, 2, 4))
```

All tasks showed some evidence of modest test-retest reliability across the school year (range=`r round(min(cors$r), digits = 2)` -- `r round(max(cors$r), digits = 2)`), comparable to the reliabilities found in our previous work (Barner et al., 2016). Higher reliability would of course increase our power to see condition effects, but might be difficult to achieve without substantially longer testing sessions. In addition, some correlations may be depressed because of real change over the course of the study. For example, we would not expect place value scores to be highly correlated given that many students learn new place value concepts over the course of the year.

We also examined intervention uptake at the end of the study (Figure 3). We found a roughly bimodial distribution of children, with some children relatively proficient at decoding abacus representations and others quite poor and only able to do so for 1 - 2 digit displays. The relative balance of children in the two modes was different across grades, however, with a much larger population of second-graders gaining proficiency in the technique.

```{r fig.width=5, fig.height = 4, fig.caption="Figure 3. Distribution of abacus uptake scores across grades. Plotting conventions are as above."}

abacus <- read_csv("data/paper/2016_AbacusFamiliarity.csv") %>%
  select(SubjectID, average) %>%
  rename(subid = SubjectID, 
         abacus_familiarity = average) %>%
  left_join(filter(d, task == "Go/No Go" & time == "Pre") %>%
              select(subid, grade)) %>%
  filter(!is.na(grade))

  
ms <- abacus %>%
  group_by(grade) %>%
  summarise(mean = mean(abacus_familiarity, na.rm=TRUE)) 

ggplot(abacus, aes(x = abacus_familiarity, fill = grade)) + 
  geom_histogram(binwidth = .1, alpha = .8) + 
  ylab("Number of Students") + 
  xlab("Abacus Familiarity Score") + 
  scale_fill_solarized(name = "Grade") + 
  geom_vline(data = ms, aes(xintercept = mean, col = grade), lty = 2) + 
  scale_color_solarized(guide = FALSE) 
```

These uptake findings are an important metric of the appropriateness of MA instruction. A relatively small proportion of first graders could accurately decode a multi-digit abacus by the end of a year of instruction (`r round(100*mean(abacus$abacus_familiarity[abacus$grade == "First Grade"] > .5))`%). Thus, MA may not have been an appropriate curriculum for these children. We discuss this result in more depth below, but we note that it qualifies the interpretation of all subsequent outcome measures for the intervention.

**Primary analyses**

The primary question addressed by our confirmatory analyses was whether assignment to treatment condition (MA vs. Control) resulted in differential change in mathematical or cognitive measures. Due to model convergence issues, we deviated from our pre-registered plan by removing random slopes for individual classes (this move follows our standard operating procedure). Table 3 shows all models, with $p$-values computed via the $t=z$ method (Barr et al., 2013). Figures 4 and 5 show scores for mathematics and cognitive tasks, respectively. 

```{r fig.cap="Table 3. Coefficients for linear mixed effects models predicting task as a function of grade, time, and intervention condition."}
models <- d %>%
  split(.$task) %>%
  map_df(function(x) {
    result <- tidy(lmer(score ~ grade + time * group + 
                (1 | subid) + 
                (1 | class_num), data = x)) %>%
      filter(group == "fixed") %>%
      select(-group) %>%
      mutate(term = fct_recode(factor(term), 
                           "Intercept" = "(Intercept)", 
                           "Second Grade" = "gradeSecondGrade", 
                           "Post-Test" = "timePost", 
                           "groupMental Abacus" = "Mental Abacus", 
                           "Post-Test x Mental Abacus" = "timePost:groupMental Abacus"))
    
    result$task <- x$task[1]
    return(result)
  })

models %>%
  select(task, term, estimate, std.error, statistic) %>%
  rename(Task = task, 
         Predictor = term, 
         Beta = estimate, 
         "Std Err" = std.error, 
         t = statistic) %>%
  mutate(p = 2*(1-pnorm(abs(t)))) %>%
  kable(digits = c(NA, NA, 3, 3, 2, 4))
```

```{r fig.width=8, fig.height = 6, fig.cap="Figure 4. Performance on mathematics measures by time and grade. Error bars show 95% confidence intervals, computed by non-parametric bootstrap."}

ms <- d %>%
  filter(task %in% c("Arithmetic","Place Value","WJ III")) %>%
  group_by(time, grade, group, task) %>%
  multi_boot_standard("score", na.rm=TRUE)

pos <- position_dodge(width = .05)
ggplot(ms, aes(x = time, y = mean, col = group, group = group)) +
  geom_line(position = pos) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = pos) + 
  facet_grid(grade~task) + 
  scale_colour_solarized(name = "Group") + 
  ylab("Mean Performance") + 
  xlab("Time Period")
```

```{r fig.width=8, fig.height = 6, fig.cap="Figure 5. Performance on cognitive measures by time and grade. Error bars show 95% confidence intervals, computed by non-parametric bootstrap."}
ms <- d %>%
  filter(task %in% c("Matrix Reasoning","Go/No Go","Spatial WM")) %>%
  group_by(time, grade, group, task) %>%
  multi_boot_standard("score", na.rm=TRUE)

pos <- position_dodge(width = .05)
ggplot(ms, aes(x = time, y = mean, col = group, group = group)) +
  geom_line(position = pos) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = pos) + 
  facet_grid(grade~task, scales = "free_y") + 
  scale_colour_solarized(name = "Group") + 
  ylab("Mean Performance") + 
  xlab("Time Period")
```

```{r}
pv_cntl <- filter(d, task == "Place Value",
              time == "Post", 
              group == "Control")$score
pv_ma <- filter(d, task == "Place Value",
              time == "Post", 
              group == "Mental Abacus")$score
pv_1s_cntl <- filter(d, task == "Place Value",
              time == "Post", 
              grade == "First Grade",
              group == "Control")$score
pv_1s_ma <- filter(d, task == "Place Value",
              time == "Post", 
              grade == "First Grade",
              group == "Mental Abacus")$score
pv_2s_cntl <- filter(d, task == "Place Value",
              time == "Post", 
              grade == "Second Grade",
              group == "Control")$score
pv_2s_ma <- filter(d, task == "Place Value",
              time == "Post", 
              grade == "Second Grade",
              group == "Mental Abacus")$score

pv.t <- t.test(pv_cntl, pv_ma)
pv.1t <- t.test(pv_1s_cntl, pv_1s_ma)
pv.2t <- t.test(pv_2s_cntl, pv_2s_ma)
```


Beginning with the math measures, we did not see numerical or statistical evidence of differential change in performance for either the in-house arithmetic or standardized WJ-III measures. This result does not replicate the findings of Barner et al. (2016), where differences on these measures emerged numerically after a single year of training. We discuss possible reasons for this disparity below. We did see a numerical trend towards the predicted time by condition interaction for the place-value measure, however. Students in the MA condition tended to make a larger gain in place value scores over the course of the study than those in the control group. This result was marginal in the mixed effects model ($p = .07$) so we interpret it with caution.^[In exploratory $t$-tests, we did see a significant post-test difference between intervention groups 
($t$(`r round(pv.t$parameter, digits=2)`) = `r round(pv.t$statistic, digits=2)`, $p$ = `r round(pv.t$p.value, digits=2)`). This test was not significant for first-graders alone, ($t$(`r round(pv.1t$parameter, digits=2)`) = `r round(pv.1t$statistic, digits=2)`, $p$ = `r round(pv.1t$p.value, digits=2)`) but was for second-graders ($t$(`r round(pv.2t$parameter, digits=2)`) = `r round(pv.2t$statistic, digits=2)`, $p$ = `r round(pv.2t$p.value, digits=2)`).] Nevertheless, it is consistent with a similar trend in Barner et al. (2016).

In the cognitive measures, we did not see evidence of differential changes in performance for either matrix reasoning or spatial working memory. These results are consistent with our previous findings and suggest again that we were unable to detect MA-related changes to spatial working memory. 

```{r fig.width=6, fig.height = 4, fig.caption="Figure 6. Reaction time on Go/No Go trials. Plotting conventions are as above."}
gonogo_raw <- read_csv("data/gonogo.csv")

mss <- gonogo_raw %>%
  filter(correct == 1) %>%
  group_by(group, grade, year, subid) %>%
  summarise(rt = mean(rt, na.rm=TRUE)) %>%
  ungroup() %>%
  select(subid, year, rt)

gonogo <- filter(d, task == "Go/No Go") %>%
  left_join(mss)

acc.cntl.mod <- lmer(score ~ rt + group + grade + time * group + 
                       (1 | subid) + 
                       (1 | class_num), data = gonogo) 

ms <- gonogo %>%
  group_by(time, grade, group) %>%
  multi_boot_standard("rt", na.rm=TRUE)

ggplot(ms, aes(x = time, y = mean, col = group, group = group)) +
  geom_line(position = pos) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper),
                  position = pos) +
  facet_grid(~grade) +
  scale_colour_solarized(name = "Group") +
  ylab("RT (ms)") +
  xlab("Time Period") +
  ylim(750,1500)
```

We did, however, find an unpredicted *negative* interaction of time and condition, such that students in the control group appeared to increase more in performance on the Go/No Go task. One possible explanation for this surprising finding would be a speed-accuracy tradeoff such that children in the MA group were less accurate but faster. This explanation appeared to be plausible based on visual inspection of the reaction times (Figure 6). To test this explanation, we performed an exploratory analysis in which we re-ran our planned linear mixed effects model on Go/No Go accuracy scores but this time including a main effect of reaction time, to control for the different average timing of participants' responses on correct trials. Consistent with the idea of a speed accuracy tradeoff, the magnitude of the time by condition interaction was now reduced by an order of magnitude and was no longer significant ($\beta = `r round(coef(summary(acc.cntl.mod))["groupMental Abacus:timePost","Estimate"], digits = 3)`$, $p = `r round(2*(1 - pnorm(abs(coef(summary(acc.cntl.mod))["groupMental Abacus:timePost","t value"]))), digits = 3)`$). Thus, we believe the Go/No Go effect reflects a shift in a speed-accuracy tradeoff rather than a true change in cognitive functioning. 

In sum, we saw at best limited evidence for the effectiveness of the MA intervention. In the math tasks, only the place value measure showed a hint of an intervention effect. And in the cognitive tasks, there were no intervention effects except for a possible shift in response criterion on the Go/No Go task. 

**Secondary analyses**

*Spatial working memory mediation analysis.*

In our previous study, we found that spatial working memory score at study initiation mediated the effects of the  intervention. Children who were above the median in spatial working memory tended to show the largest gains in arithmetic performance from studying MA. We replicated this analysis on all three of our math measures (Figure 7). Of the three, only place value showed the predicted pattern, and only for the second graders. Numerically, the pattern for place value was similar to what we observed in the arithmetic measure in our first study: greater growth for high spatial WM abacus users. But in exploratory models, the three-way interaction of spatial working memory, time, and condition was not significant. Likely our study would have required considerably more power to detect such an effect. 

```{r fig.width=8, fig.height = 6, fig.caption="Figure 7. Accuracies for math tasks, split by intervention group and spatial working memory at initiation. Plotting conventions are as above."}
high_swm <- d %>%
  filter(year == "2015" & task == "Spatial WM") %>%
  group_by(grade) %>%
  mutate(high_swm = score > median(score, na.rm=TRUE)) %>%
  filter(high_swm)

mss <- d %>%
  mutate(high_swm = subid %in% high_swm$subid) %>%
  filter(task %in% c("Arithmetic", "Place Value", "WJ III")) 

ms <- mss %>%
  group_by(time, group, grade, task, high_swm) %>%
  multi_boot_standard("score", na.rm=TRUE)

pos <- position_dodge(width = .05)
ggplot(ms, aes(x = time, y = mean, col = group, pch = high_swm, lty = high_swm)) +
  geom_line(position = pos, aes(group = interaction(high_swm, group))) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper), 
                  position = pos) + 
  facet_grid(grade~task) + 
  ylab("Mean Performance") + 
  xlab("Time Period") +
  scale_colour_solarized(name = "Group") + 
  scale_linetype_discrete(name = "High Spatial Working Memory")

swm.mod <- lmer(score ~ group + grade + time * group * high_swm + 
                       (1 | subid) + 
                       (1 | class_num), data = filter(mss, task == "Place Value")) 

swm.2.mod <- lmer(score ~ group + time * group * high_swm+ 
                       (1 | subid) + 
                       (1 | class_num), data = filter(mss, 
                                                      grade == "Second Grade",
                                                      task == "Place Value")) 
```

*Math anxiety*

We further assessed whether the MA intervention led to changes in math anxiety at the end of the study. As shown in Figure 8, though first-graders showed overall more math anxiety than second graders, there were only minor numerical differences in math anxiety between groups. 

```{r fig.width=5, fig.height = 4, fig.caption="Figure 8. Math anxiety by grade and group. Error bars represent 95% confidence intervals, computed by non-parametric bootstrap."}

anxiety <- read_csv("data/paper/2016_MathAnxiety.csv") %>%
  select(SubjectID, average) %>%
  rename(subid = SubjectID, 
         math_anxiety = average) %>%
  left_join(filter(d, task == "Go/No Go" & time == "Pre") %>%
              select(subid, grade, group)) %>%
  filter(!is.na(grade))

ms <- anxiety %>%
  group_by(grade, group) %>%
  multi_boot_standard(col = "math_anxiety", na.rm=TRUE)

ggplot(ms, aes(x = grade, y = mean, fill = group)) + 
  geom_bar(stat = "identity", position = "dodge") +
  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper), 
                 position = position_dodge(width = .9)) +
  scale_fill_solarized(name = "Group") +
  ylab("Self-Rated Math Anxiety") + 
  xlab("Grade") + 
  ylim(0,5)
```

